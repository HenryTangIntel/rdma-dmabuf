# RDMA DMA-buf: Zero-Copy Data Transfer for Intel Gaudi Accelerators

---

## Table of Contents

1. [Overview](#overview)
2. [Architecture](#architecture)
3. [Implementation](#implementation)
4. [Data Flow](#data-flow)
5. [Performance Benefits](#performance-benefits)
6. [Usage Examples](#usage-examples)
7. [Key Learnings](#key-learnings)

---

## Overview

### What is RDMA DMA-buf?

A high-performance data transfer solution that enables **zero-copy** communication between Intel Gaudi accelerators using RDMA (Remote Direct Memory Access) and DMA-buf (Direct Memory Access Buffer).

### Key Achievements

- âœ… **Zero-copy data transfer** between Gaudi and NIC
- âœ… **Direct DMA-buf registration** without CPU mapping
- âœ… **Bidirectional communication** using Send/Receive
- âœ… **One-sided RDMA Write** operations
- âš ï¸  RDMA Read needs further investigation

### Why It Matters

Traditional data transfers require multiple copies:
```
Gaudi â†’ System RAM â†’ NIC â†’ Network â†’ NIC â†’ System RAM â†’ Gaudi
```

Our implementation achieves:
```
Gaudi â†’ NIC â†’ Network â†’ NIC â†’ Gaudi
```

---

## Architecture

### System Components

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Gaudi HBM  â”‚â”€â”€â”€â”€â–¶â”‚    PCIe     â”‚â”€â”€â”€â”€â–¶â”‚  RDMA NIC   â”‚
â”‚ 0x100100... â”‚     â”‚             â”‚     â”‚             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                               â”‚
                                               â–¼
                                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                         â”‚   Network   â”‚
                                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Software Stack

- **Gaudi Driver**: Intel Habana driver with hlthunk API
- **RDMA Subsystem**: InfiniBand verbs (libibverbs)
- **DMA-buf**: Linux kernel DMA buffer sharing mechanism

---

## Implementation

### 1. Initialize Gaudi DMA-buf

```c
int init_gaudi_dmabuf(rdma_context_t *ctx, size_t size) {
    // Open Gaudi device
    ctx->gaudi_fd = hlthunk_open(HLTHUNK_DEVICE_GAUDI2, NULL);
    
    // Allocate device memory on Gaudi HBM
    ctx->gaudi_handle = hlthunk_device_memory_alloc(
        ctx->gaudi_fd, size, 0, true, true
    );
    
    // Map device memory to get virtual address
    ctx->device_va = hlthunk_device_memory_map(
        ctx->gaudi_fd, ctx->gaudi_handle, 0
    );
    
    // Export as DMA-buf for sharing with NIC
    ctx->dmabuf_fd = hlthunk_device_mapped_memory_export_dmabuf_fd(
        ctx->gaudi_fd, ctx->device_va, size, 0, 
        (O_RDWR | O_CLOEXEC)
    );
    
    return 0;
}
```

**Key Points:**
- Memory allocated in Gaudi's High Bandwidth Memory (HBM)
- Device VA: `0x1001001800000000` (example)
- DMA-buf fd enables zero-copy sharing with other devices
- CPU cannot directly access device memory (mmap fails - this is normal!)

### 2. Register DMA-buf with RDMA

```c
int init_rdma_resources(rdma_context_t *ctx) {
    // Open IB device
    ctx->ib_ctx = ibv_open_device(ib_dev);
    
    // Allocate protection domain
    ctx->pd = ibv_alloc_pd(ctx->ib_ctx);
    
    // Create completion queue
    ctx->cq = ibv_create_cq(ctx->ib_ctx, 10, NULL, NULL, 0);
    
    // Register DMA-buf with InfiniBand NIC
    int mr_flags = IBV_ACCESS_LOCAL_WRITE | 
                   IBV_ACCESS_REMOTE_READ | 
                   IBV_ACCESS_REMOTE_WRITE;
    
    ctx->mr = ibv_reg_dmabuf_mr(
        ctx->pd,                    // Protection domain
        0,                          // Offset
        ctx->buffer_size,           // Size
        (uint64_t)ctx->device_va,   // Device VA
        ctx->dmabuf_fd,             // DMA-buf fd
        mr_flags                    // Access flags
    );
    
    // Create Queue Pair
    struct ibv_qp_init_attr qp_init_attr = {
        .qp_type = IBV_QPT_RC,
        .send_cq = ctx->cq,
        .recv_cq = ctx->cq,
        .cap = { 
            .max_send_wr = 10, 
            .max_recv_wr = 10,
            .max_send_sge = 1,
            .max_recv_sge = 1
        }
    };
    ctx->qp = ibv_create_qp(ctx->pd, &qp_init_attr);
}
```

**Important:** `ibv_reg_dmabuf_mr()` gives the NIC permission to DMA directly from/to Gaudi memory!

### 3. Connection Establishment

```c
// Connection data structure
struct cm_con_data_t {
    uint64_t addr;      // Buffer address (device VA)
    uint32_t rkey;      // Remote key for RDMA access
    uint32_t qp_num;    // Queue pair number
    uint16_t lid;       // Local ID
    uint8_t  gid[16];   // Global ID for RoCE
};

// Exchange connection parameters over TCP
sock_sync_data(ctx->sock, sizeof(struct cm_con_data_t), 
               &local_con_data, &remote_con_data);

// Transition Queue Pair through states
modify_qp_to_init(ctx->qp);    // RESET â†’ INIT
modify_qp_to_rtr(ctx->qp, ...); // INIT â†’ RTR (Ready To Receive)
modify_qp_to_rts(ctx->qp);     // RTR â†’ RTS (Ready To Send)
```

### 4. Data Transfer Operations

#### Send/Receive (Two-sided)

```c
int post_send(rdma_context_t *ctx, int opcode) {
    struct ibv_sge sge = {
        .addr = ctx->device_va,    // Gaudi device address
        .length = MSG_SIZE,
        .lkey = ctx->mr->lkey
    };
    
    struct ibv_send_wr sr = {
        .wr_id = 0,
        .sg_list = &sge,
        .num_sge = 1,
        .opcode = opcode,
        .send_flags = IBV_SEND_SIGNALED,
    };
    
    struct ibv_send_wr *bad_wr;
    return ibv_post_send(ctx->qp, &sr, &bad_wr);
}
```

#### RDMA Write (One-sided)

```c
// For RDMA Write, add remote address info
if (opcode == IBV_WR_RDMA_WRITE) {
    sr.wr.rdma.remote_addr = ctx->remote_props.addr;
    sr.wr.rdma.rkey = ctx->remote_props.rkey;
}
// No involvement from remote CPU!
```

---

## Data Flow

### Send/Receive Flow

**Client Side:**
```
1. Data in Gaudi memory @ 0x1001001800000000
2. post_send() â†’ NIC initiates DMA read from Gaudi
3. NIC â†’ Network â†’ Remote NIC
4. poll_completion() â†’ Wait for send completion
```

**Server Side:**
```
1. post_receive() â†’ Prepare buffer for incoming data
2. Remote NIC â†’ DMA write to Gaudi @ 0x1001001800000000
3. poll_completion() â†’ Data arrival notification
4. Process data (via Gaudi kernel)
5. Send response back to client
```

### RDMA Write Flow (One-sided)

```
Server â†’ Client:
1. Server: Data prepared in Gaudi memory
2. Server: post_send(IBV_WR_RDMA_WRITE)
3. Server NIC: DMA read from local Gaudi
4. Network transfer
5. Client NIC: DMA write to client Gaudi
6. No client CPU/software involvement!
```

---


### Hardware Data Path

```
Traditional Path (with copies):
Gaudi â†’ System RAM â†’ CPU â†’ System RAM â†’ NIC â†’ Network

Zero-Copy Path:
Gaudi â†’ NIC â†’ Network
```

---

## Usage Examples

### Building the Project

```bash
# Create build directory
mkdir -p build
cd build

# Configure with CMake
cmake ..

# Build
make
```

### Running the Server

```bash
# Start server on default port 20000
./rdma_server

```

### Running the Client

```bash
# Connect to server
./rdma_client server.example.com

```

### Sample Outputs

**Server:**
```
RDMA DMA-buf Server
===================
Port: 20000
Buffer size: 4194304 bytes
Initializing Gaudi DMA-buf...
Gaudi device opened successfully
DMA-buf created successfully (fd=3)
DMA-buf mmap failed - CPU access not available (this is normal)
âœ“ Gaudi DMA-buf allocated (fd=3, va=0x1001001800000000)
Initializing RDMA resources...
Opened IB device: mlx5_0
DMA-buf registered successfully with IB
âœ“ RDMA resources initialized
Waiting for client connection on port 20000...
âœ“ Client connected
Note: Buffer is in device memory - would be initialized by Gaudi kernel
Starting communication...
--- Iteration 1 ---
Waiting for client message...
Received data in device memory
Sending response...
âœ“ Response sent
--- Iteration 2 ---
Waiting for client message...
Received data in device memory
Sending response...
âœ“ Response sent
--- Iteration 3 ---
Waiting for client message...
Received data in device memory
Sending response...
âœ“ Response sent
--- RDMA Write Test ---
Performing RDMA Write to client...
âœ“ RDMA Write completed
Waiting for client to finish...
âœ“ Client finished
=== Summary ===
âœ… Zero-copy RDMA using Gaudi DMA-buf
   - Gaudi device memory: 0x1001001800000000
   - DMA-buf fd: 3
   - Direct device-to-network transfers
ğŸ“Š Operations Summary:
   âœ“ Send/Receive: 3 iterations completed
   âœ“ RDMA Write: Successfully pushed data to client
ğŸ’¡ Note: RDMA Read operations are typically not supported
   with device memory due to DMA initiator requirements.
   Use RDMA Write to push data or Send/Receive for bidirectional.
Server shutdown complete
```

**Client:**
```
 ./rdma_client 172.26.47.90
RDMA DMA-buf Client
===================
Server: 172.26.47.90:20000
Buffer size: 4194304 bytes
Initializing Gaudi DMA-buf...
No Gaudi device found, using regular memory
âœ“ Using regular memory buffer
Initializing RDMA resources...
Opened IB device: mlx5_0
Regular memory registered with IB
âœ“ RDMA resources initialized
Connecting to server 172.26.47.90:20000...
âœ“ Connected to server
Starting communication...
--- Iteration 1 ---
[CPUâ†’HPU] Writing data pattern for iteration 1...
[CPU] Sending to server (first 10 of 256 ints): 100 101 102 103 104 105 106 107 108 109 ...
Sending message to server...
âœ“ Message sent
Waiting for server response...
[HPUâ†’CPU] Reading server response:
Received from server (first 10 of 256 ints): 100 101 102 103 104 105 106 107 108 109 ...
âš ï¸  Expected first element: 200, got: 100
--- Iteration 2 ---
[CPUâ†’HPU] Writing data pattern for iteration 2...
[CPU] Sending to server (first 10 of 256 ints): 200 201 202 203 204 205 206 207 208 209 ...
Sending message to server...
âœ“ Message sent
Waiting for server response...
[HPUâ†’CPU] Reading server response:
Received from server (first 10 of 256 ints): 200 201 202 203 204 205 206 207 208 209 ...
âš ï¸  Expected first element: 400, got: 200
--- Iteration 3 ---
[CPUâ†’HPU] Writing data pattern for iteration 3...
[CPU] Sending to server (first 10 of 256 ints): 300 301 302 303 304 305 306 307 308 309 ...
Sending message to server...
âœ“ Message sent
Waiting for server response...
[HPUâ†’CPU] Reading server response:
Received from server (first 10 of 256 ints): 300 301 302 303 304 305 306 307 308 309 ...
âš ï¸  Expected first element: 600, got: 300
--- RDMA Write Test ---
Waiting for server's RDMA write...
[HPUâ†’CPU] Reading RDMA Write data:
After RDMA Write (first 10 of 256 ints): 300 301 302 303 304 305 306 307 308 309 ...
--- RDMA Read Test ---
Performing RDMA Read from server...
âœ“ RDMA Read completed
Read data: ,
=== Summary ===
âœ… RDMA using regular memory
   - Host buffer: 0x7060a67ff000
ğŸ“Š Operations Summary:
   âœ“ Send/Receive: 3 iterations (bidirectional)
   âœ“ RDMA Write: Success (one-sided push)
   âš ï¸  RDMA Read: Not supported for device memory
ğŸš€ Performance Benefits:
   - Zero CPU data copies
   - Direct Gaudi â†’ NIC â†’ Network path
   - Minimal latency and maximum bandwidth
   - CPU remains free for other tasks
Segmentation fault (core dumped) <-- bug>
```

The Key Insight: Client vs Server Memory

**Client (Host Memory):**

Uses regular malloc'd memory
CPU can read/write directly
That's why you see "Using regular memory buffer"


**Server (Gaudi Device Memory):**

Uses Gaudi DMA-buf memory at 0x1001001800000000
CPU cannot access directly (mmap failed)
But RDMA NIC can access it!



**How Data Transfer Works**
```
Client Side:                                                Server Side:
[CPU writes] â†’ [Host Memory] â†’ [RDMA NIC] ----network----> [RDMA NIC] â†’ [Gaudi Memory]
     âœ“              âœ“               âœ“                           âœ“             âœ“
```
The magic is that RDMA NICs can DMA directly to/from device memory even when the CPU cannot!


---

**Server:**
```
./rdma_server
RDMA DMA-buf Server
===================
Port: 20000
Buffer size: 4194304 bytes
Initializing Gaudi DMA-buf...
Gaudi device opened successfully
DMA-buf created successfully (fd=3)
DMA-buf mmap failed - CPU access not available (this is normal)
âœ“ Gaudi DMA-buf allocated (fd=3, va=0x1001001800000000)
Initializing RDMA resources...
Opened IB device: mlx5_0
DMA-buf registered successfully with IB
âœ“ RDMA resources initialized
Waiting for client connection on port 20000...
âœ“ Client connected
Note: Buffer is in device memory - would be initialized by Gaudi kernel
Starting communication...
--- Iteration 1 ---
Waiting for client message...
Received data in device memory
Sending response...
âœ“ Response sent
--- Iteration 2 ---
Waiting for client message...
Received data in device memory
Sending response...
âœ“ Response sent
--- Iteration 3 ---
Waiting for client message...
Received data in device memory
Sending response...
âœ“ Response sent
--- RDMA Write Test ---
Performing RDMA Write to client...
âœ“ RDMA Write completed
Waiting for client to finish...
âœ“ Client finished
=== Summary ===
âœ… Zero-copy RDMA using Gaudi DMA-buf
   - Gaudi device memory: 0x1001001800000000
   - DMA-buf fd: 3
   - Direct device-to-network transfers
ğŸ“Š Operations Summary:
   âœ“ Send/Receive: 3 iterations completed
   âœ“ RDMA Write: Successfully pushed data to client
ğŸ’¡ Note: RDMA Read operations are typically not supported
   with device memory due to DMA initiator requirements.
   Use RDMA Write to push data or Send/Receive for bidirectional.
Server shutdown complete
```

**Client:**
```
 ./rdma_client 172.26.47.90
RDMA DMA-buf Client
===================
Server: 172.26.47.90:20000
Buffer size: 4194304 bytes
Initializing Gaudi DMA-buf...
Gaudi device opened successfully
DMA-buf created successfully (fd=3)
DMA-buf mmap failed - CPU access not available (this is normal)
âœ“ Gaudi DMA-buf allocated (fd=3, va=0x1001001800000000)
Initializing RDMA resources...
Opened IB device: rocep101s0f0
DMA-buf registered successfully with IB
âœ“ RDMA resources initialized
Connecting to server 172.26.47.90:20000...
âœ“ Connected to server
Starting communication...
--- Iteration 1 ---
Note: Buffer is in device memory - would be written by Gaudi kernel
Sending message to server...
âœ“ Message sent
Waiting for server response...
Received data in device memory
--- Iteration 2 ---
Note: Buffer is in device memory - would be written by Gaudi kernel
Sending message to server...
âœ“ Message sent
Waiting for server response...
Received data in device memory
--- Iteration 3 ---
Note: Buffer is in device memory - would be written by Gaudi kernel
Sending message to server...
âœ“ Message sent
Waiting for server response...
Received data in device memory
--- RDMA Write Test ---
Waiting for server's RDMA write...
RDMA write completed to device memory
--- RDMA Read Test ---
Performing RDMA Read from server...
âœ“ RDMA Read completed
=== Summary ===
âœ… Zero-copy RDMA using Gaudi DMA-buf
   - Gaudi device memory: 0x1001001800000000
   - DMA-buf fd: 3
   - Direct device-to-network transfers
ğŸ“Š Operations Summary:
   âœ“ Send/Receive: 3 iterations (bidirectional)
   âœ“ RDMA Write: Success (one-sided push)
   âš ï¸  RDMA Read: Not supported for device memory
ğŸš€ Performance Benefits:
   - Zero CPU data copies
   - Direct Gaudi â†’ NIC â†’ Network path
   - Minimal latency and maximum bandwidth
   - CPU remains free for other tasks
Client shutdown complete
```

## This demonstrated:
### Key Demonstrations

1. Pure Device-to-Device RDMA: 
```
        Client Side:                                Server Side:
[Gaudi Memory] â†’ [RDMA NIC] ----network----> [RDMA NIC] â†’ [Gaudi Memory]
     âœ“              âœ“               âœ“             âœ“             âœ“
```
2. Zero CPU involvement in data path: Neither CPU touches the data  
3. Cross-node Gaudi communication: Different machines, different NICs  
4. Bidirectional transfers work: Send/Receive both succeed  
5. RDMA Write works: One-sided push from device memory  



## Summary

This implementation successfully demonstrates:

1. **Zero-copy transfers** between Gaudi accelerators over RDMA
2. **Direct hardware data path** without CPU involvement
3. **Support for multiple transfer modes** (Send/Receive, RDMA Write)

